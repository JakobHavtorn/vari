<!doctype html>
<meta charset="utf-8">
<script src="https://distill.pub/template.v1.js"></script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>


<!-- Front matter -->
<script type="text/front-matter">
  title: "Knowing what we don't know"
  description: "A close look at the differences of discriminative and generative modelling."
  authors:
  - Jakob Havtorn: http://github.com/JakobHavtorn
  - Lars Maaløe: https://github.com/larsmaaloee
  affiliations:
  - Corti.ai: http://corti.ai
  - Corti.ai: http://corti.ai
</script>

<dt-article>
    <h1>Knowing what we don't know</h1>

    <h2>A close look at the differences of discriminative and generative modelling.</h2>
    
    <dt-byline></dt-byline>

    <p>
    Modern machine learning is dominated by the use of large discriminative models....
    </p>

    <hr>

    <h2>The fundamental failure of discriminative models</h2>

    <h2>The supposed failure of generative models</h2>

    <h2>Header 2</h2>
    <!-- <p>
        Recently, generative models have been shown to sometimes assign similar or higher likelihoods to out-of-distribution data than they do to the training data <dt-cite key="Nalisnick2019"></dt-cite> <dt-cite key="Nalisnick2018"></dt-cite>. Similarly to the above, we argue the this is also in part due to models having to low capacity. Additionally, we argue that such likelihoods for out-of-distribution data can be seen as being due to recognition of well-known low-level features in the encoder model. For instance, a model trained on FashionMNIST might assign similar or higher likelihood for MNIST than for FashionMNIST due to the datasets sharing low-level features such as edge-detectors.
    </p> -->

    <p>
        Variational autoencoders are known to fail at generation in two specific ways. The first way relates to intrinsic problems with using the ELBO as an objective function <dt-cite key="TODO"></dt-cite>, while the second is related to pockets of low density in the aggregate posterior where the prior has high density <dt-cite key="Bhalodia2019"></dt-cite>.
    </p>

    <p>
        We argue that the formation of low-likelihood pockets in the aggregate posterior during training of a VAE can be seen as a consequence of using a model with too little capacity/flexibility. We find it highly plausbile that by constructing a hierarchy of latent variables, such as in a 2 layer VAE, where the priors are conditional on each other, the model can become flexible enough to not form such latent pockets. 
    </p>

    <p>
        Another way of adding more flexibility to the model is to include prior knowledge. FOr the Moons or Spirals synthetic datasets, we know <em>a priori</em> that the latent space would benefit from having a bi-modal distribution since there are effectively two classes being modelled. This can be achieved by defining the prior as a mixture of two Gaussians $p(\mathbf{z}) = \frac{1}{2}\mathcal{N}(\mathbf{z}|\mathbf{1},\mathbf{I}) + \frac{1}{2}\mathcal{N}(\mathbf{z}|\mathbf{-\mathbf{1}},\mathbf{I})$
    </p>


    <p>
        <b>Latent pockets in relation to top-down and bottom-up inference</b>:
        Bottom-up inference paths have the latent space with the simple prior distribution sitting at the highest point in the hierarchy of latent variables $q(\mathbf{z}_L|\mathbf{z}_{ < L})$. This intuitively allows encoding abstract features into the simple prior due to the high flexibility of the underlying model. In top-down inference paths, the top-most latent variable is the first to be inferred from the data $q(\mathbf{z}_L|\mathbf{x})$. In models using such top-down inference paths we must expect the problem of latent pockets forming between the aggregate posterior in that top-most latent variable $\int q(\mathbf{z}_L|\mathbf{x}) d\mathbf{x} = q(\mathbf{z}_L)$ and its prior $\mathcal{N}(\mathbf{z}|\mathbf{0}, \mathbf{I})$ to be especially pronounced.
    </p>
    
    <p>
      <b>Novelty contribution</b>
      
      Can we find a good measure of when a model is trained to be a good OOD detector? Can we say that the model "overfits" and fails at OOD when the measure is exceeded in some way?
      
      <ul>
        <li> If ELBO is too low when this is exceeded --> You need more expressive model e.g. more latents.
        <li> If ELBO and KL at top layer are both high then the model is "overfitted" and OOD detection is poor. A test
      </ul> 
    </p>

    <h3>Header 3</h3>

    <h4>Header 4</h4>
    
    <h5>Header 5</h5>

    <h6>Header 6</h6>


    <p>This is the first paragraph of the article.<dt-fn>This will become a hoverable footnote.</dt-fn>

    <p>Or do math like this:
        When $a \ne 0$, there are two solutions to \(ax^2 + bx + c = 0\) and they are

        $$x = {-b \pm \sqrt{b^2-4ac} \over 2a}.$$
    </p>



    <!-- Code block -->
    <p>Here is an example of some sweet javascript</p>
    <dt-code block language="javascript">
        var x = 25;
        function(x){
        return x * x;
        }
    </dt-code>


</dt-article>

<dt-appendix>
</dt-appendix>

<!-- Bibliography -->
<script type="text/bibliography">

    @article{Bhalodia2019,
        title = {{{dpVAEs}}: {{Fixing Sample Generation}} for {{Regularized VAEs}}},
        shorttitle = {{{dpVAEs}}},
        author = {Bhalodia, Riddhish and Lee, Iain and Elhabian, Shireen},
        date = {2019-11-24},
        url = {http://arxiv.org/abs/1911.10506},
        urldate = {2019-12-20},
        abstract = {Unsupervised representation learning via generative modeling is a staple to many computer vision applications in the absence of labeled data. Variational Autoencoders (VAEs) are powerful generative models that learn representations useful for data generation. However, due to inherent challenges in the training objective, VAEs fail to learn useful representations amenable for downstream tasks. Regularization-based methods that attempt to improve the representation learning aspect of VAEs come at a price: poor sample generation. In this paper, we explore this representation-generation trade-off for regularized VAEs and introduce a new family of priors, namely decoupled priors, or dpVAEs, that decouple the representation space from the generation space. This decoupling enables the use of VAE regularizers on the representation space without impacting the distribution used for sample generation, and thereby reaping the representation learning benefits of the regularizations without sacrificing the sample generation. dpVAE leverages invertible networks to learn a bijective mapping from an arbitrarily complex representation distribution to a simple, tractable, generative distribution. Decoupled priors can be adapted to the state-of-the-art VAE regularizers without additional hyperparameter tuning. We showcase the use of dpVAEs with different regularizers. Experiments on MNIST, SVHN, and CelebA demonstrate, quantitatively and qualitatively, that dpVAE fixes sample generation for regularized VAEs.},
        archivePrefix = {arXiv},
        eprint = {1911.10506},
        eprinttype = {arxiv},
        file = {/Users/jakob/Documents/Zotero/Bhalodia et al - 2019 - dpVAEs - Fixing Sample Generation for Regularized VAEs.pdf},
        keywords = {⛔ No DOI found,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
        langid = {english},
        primaryClass = {cs, stat}
      }
      
      @article{Burda2015,
        title = {Importance {{Weighted Autoencoders}}},
        author = {Burda, Yuri and Grosse, Roger and Salakhutdinov, Ruslan R.},
        date = {2015},
        pages = {8},
        publisher = {{University of Toronto}},
        issn = {1312.6114v10},
        url = {https://arxiv.org/abs/1509.00519},
        urldate = {2017-10-04},
        abstract = {The variational autoencoder (VAE; Kingma, Welling (2014)) is a recently proposed generative model pairing a top-down generative network with a bottom-up recognition network which approximates posterior inference. It typically makes strong assumptions about posterior inference, for instance that the posterior distribution is approximately factorial, and that its parameters can be approximated with nonlinear regression from the observations. As we show empirically, the VAE objective can lead to overly simplified representations which fail to use the network's entire modeling capacity. We present the importance weighted autoencoder (IWAE), a generative model with the same architecture as the VAE, but which uses a strictly tighter log-likelihood lower bound derived from importance weighting. In the IWAE, the recognition network uses multiple samples to approximate the posterior, giving it increased flexibility to model complex posteriors which do not fit the VAE modeling assumptions. We show empirically that IWAEs learn richer latent space representations than VAEs, leading to improved test log-likelihood on density estimation benchmarks.},
        archivePrefix = {arXiv},
        eprint = {1509.00519},
        eprinttype = {arxiv},
        file = {/Users/jakob/Documents/Zotero/Burda et al - 2015 - Importance Weighted Autoencoders.pdf},
        isbn = {1509.00519},
        place = {Toronto}
      }
      
      @article{Cremer2018,
        title = {Inference {{Suboptimality}} in {{Variational Autoencoders}}},
        author = {Cremer, Chris and Li, Xuechen and Duvenaud, David},
        date = {2018-05-27},
        url = {http://arxiv.org/abs/1801.03558},
        urldate = {2020-01-14},
        abstract = {Amortized inference allows latent-variable models trained via variational learning to scale to large datasets. The quality of approximate inference is determined by two factors: a) the capacity of the variational distribution to match the true posterior and b) the ability of the recognition network to produce good variational parameters for each datapoint. We examine approximate inference in variational autoencoders in terms of these factors. We find that divergence from the true posterior is often due to imperfect recognition networks, rather than the limited complexity of the approximating distribution. We show that this is due partly to the generator learning to accommodate the choice of approximation. Furthermore, we show that the parameters used to increase the expressiveness of the approximation play a role in generalizing inference rather than simply improving the complexity of the approximation.},
        archivePrefix = {arXiv},
        eprint = {1801.03558},
        eprinttype = {arxiv},
        file = {/Users/jakob/Zotero/storage/PZG6G5Y7/Cremer et al. - 2018 - Inference Suboptimality in Variational Autoencoder.pdf},
        keywords = {⛔ No DOI found,Computer Science - Machine Learning,Statistics - Machine Learning},
        langid = {english},
        primaryClass = {cs, stat}
      }
      
      @article{Kingma2013,
        title = {Auto-{{Encoding Variational Bayes}}},
        author = {Kingma, Diederik P and Welling, Max},
        date = {2013-12-20},
        journaltitle = {arXiv preprint},
        url = {http://arxiv.org/abs/1312.6114},
        urldate = {2018-06-07},
        abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
        archivePrefix = {arXiv},
        eprint = {1312.6114},
        eprinttype = {arxiv},
        file = {/Users/jakob/Documents/Zotero/Kingma, Welling - 2013 - Auto-Encoding Variational Bayes.pdf},
        keywords = {★}
      }
      
      @article{Maaloe2016,
        title = {Auxiliary {{Deep Generative Models}}},
        author = {Maaløe, Lars and Sønderby, Casper Kaae and Sønderby, Søren Kaae and Winther, Ole},
        date = {2016-06-16},
        url = {http://arxiv.org/abs/1602.05473},
        urldate = {2019-12-15},
        abstract = {Deep generative models parameterized by neural networks have recently achieved state-ofthe-art performance in unsupervised and semisupervised learning. We extend deep generative models with auxiliary variables which improves the variational approximation. The auxiliary variables leave the generative model unchanged but make the variational distribution more expressive. Inspired by the structure of the auxiliary variable we also propose a model with two stochastic layers and skip connections. Our findings suggest that more expressive and properly specified deep generative models converge faster with better results. We show state-of-theart performance within semi-supervised learning on MNIST, SVHN and NORB datasets.},
        archivePrefix = {arXiv},
        eprint = {1602.05473},
        eprinttype = {arxiv},
        file = {/Users/jakob/Documents/Zotero/Maaløe et al - 2016 - Auxiliary Deep Generative Models.pdf},
        keywords = {⛔ No DOI found,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
        langid = {english},
        primaryClass = {cs, stat}
      }
      
      @article{Maaloe2019,
        title = {{{BIVA}}: {{A Very Deep Hierarchy}} of {{Latent Variables}} for {{Generative Modeling}}},
        shorttitle = {{{BIVA}}},
        author = {Maaløe, Lars and Fraccaro, Marco and Liévin, Valentin and Winther, Ole},
        date = {2019-02-06},
        url = {http://arxiv.org/abs/1902.02102},
        urldate = {2019-03-19},
        abstract = {With the introduction of the variational autoencoder (VAE), probabilistic latent variable models have received renewed attention as powerful generative models. However, their performance in terms of test likelihood and quality of generated samples has been surpassed by autoregressive models without stochastic units. Furthermore, flow-based models have recently been shown to be an attractive alternative that scales well to high-dimensional data. In this paper we close the performance gap by constructing VAE models that can effectively utilize a deep hierarchy of stochastic variables and model complex covariance structures. We introduce the Bidirectional-Inference Variational Autoencoder (BIVA), characterized by a skip-connected generative model and an inference network formed by a bidirectional stochastic inference path. We show that BIVA reaches state-of-the-art test likelihoods, generates sharp and coherent natural images, and uses the hierarchy of latent variables to capture different aspects of the data distribution. We observe that BIVA, in contrast to recent results, can be used for anomaly detection. We attribute this to the hierarchy of latent variables which is able to extract high-level semantic features. Finally, we extend BIVA to semi-supervised classification tasks and show that it performs comparably to state-of-the-art results by generative adversarial networks.},
        archivePrefix = {arXiv},
        eprint = {1902.02102},
        eprinttype = {arxiv},
        file = {/Users/jakob/Documents/Zotero/Maaløe et al - 2019 - BIVA - A Very Deep Hierarchy of Latent Variables for Generative Modeling.pdf;/Users/jakob/Documents/Zotero/Maaløe et al - 2019 - BIVA - A Very Deep Hierarchy of Latent Variables for Generative Modeling2.pdf;/Users/jakob/Documents/Zotero/Maaløe et al - 2019 - BIVA - A Very Deep Hierarchy of Latent Variables for Generative Modeling3.pdf;/Users/jakob/Zotero/storage/HMKGHSU7/1902.html},
        keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
        primaryClass = {cs, stat}
      }
      
      @article{Nalisnick2019,
        title = {Detecting {{Out}}-of-{{Distribution Inputs}} to {{Deep Generative Models Using Typicality}}},
        author = {Nalisnick, Eric and Matsukawa, Akihiro and Teh, Yee Whye and Lakshminarayanan, Balaji},
        pages = {15},
        file = {/Users/jakob/Documents/Zotero/Nalisnick et al - - Detecting Out-of-Distribution Inputs to Deep Generative Models Using Typicality.pdf},
        keywords = {⛔ No DOI found},
        langid = {english}
      }
      
      @article{Nalisnick2018,
        title = {Do {{Deep Generative Models Know What They Don}}'t {{Know}}?},
        author = {Nalisnick, Eric and Matsukawa, Akihiro and Teh, Yee Whye and Gorur, Dilan and Lakshminarayanan, Balaji},
        date = {2018-10-22},
        url = {http://arxiv.org/abs/1810.09136},
        urldate = {2019-10-02},
        abstract = {A neural network deployed in the wild may be asked to make predictions for inputs that were drawn from a different distribution than that of the training data. A plethora of work has demonstrated that it is easy to find or synthesize inputs for which a neural network is highly confident yet wrong. Generative models are widely viewed to be robust to such mistaken confidence as modeling the density of the input features can be used to detect novel, out-of-distribution inputs. In this paper we challenge this assumption. We find that the density learned by flow-based models, VAEs, and PixelCNNs cannot distinguish images of common objects such as dogs, trucks, and horses (i.e. CIFAR-10) from those of house numbers (i.e. SVHN), assigning a higher likelihood to the latter when the model is trained on the former. Moreover, we find evidence of this phenomenon when pairing several popular image data sets: FashionMNIST vs MNIST, CelebA vs SVHN, ImageNet vs CIFAR-10 / CIFAR-100 / SVHN. To investigate this curious behavior, we focus analysis on flow-based generative models in particular since they are trained and evaluated via the exact marginal likelihood. We find such behavior persists even when we restrict the flows to constant-volume transformations. These transformations admit some theoretical analysis, and we show that the difference in likelihoods can be explained by the location and variances of the data and the model curvature. Our results caution against using the density estimates from deep generative models to identify inputs similar to the training distribution until their behavior for out-of-distribution inputs is better understood.},
        archivePrefix = {arXiv},
        eprint = {1810.09136},
        eprinttype = {arxiv},
        file = {/Users/jakob/Documents/Zotero/Nalisnick et al - 2018 - Do Deep Generative Models Know What They Don't Know.pdf},
        keywords = {⛔ No DOI found,Computer Science - Machine Learning,Statistics - Machine Learning},
        langid = {english},
        primaryClass = {cs, stat}
      }
      
      @article{Sonderby2016,
        title = {Ladder {{Variational Autoencoders}}},
        author = {Sønderby, Casper Kaae and Raiko, Tapani and Maaløe, Lars and Sønderby, Søren Kaae and Winther, Ole},
        date = {2016-05-27},
        url = {http://arxiv.org/abs/1602.02282},
        urldate = {2019-11-15},
        abstract = {Variational autoencoders are powerful models for unsupervised learning. However deep models with several layers of dependent stochastic variables are difficult to train which limits the improvements obtained using these highly expressive models. We propose a new inference model, the Ladder Variational Autoencoder, that recursively corrects the generative distribution by a data dependent approximate likelihood in a process resembling the recently proposed Ladder Network. We show that this model provides state of the art predictive log-likelihood and tighter log-likelihood lower bound compared to the purely bottom-up inference in layered Variational Autoencoders and other generative models. We provide a detailed analysis of the learned hierarchical latent representation and show that our new inference model is qualitatively different and utilizes a deeper more distributed hierarchy of latent variables. Finally, we observe that batch normalization and deterministic warm-up (gradually turning on the KL-term) are crucial for training variational models with many stochastic layers.},
        archivePrefix = {arXiv},
        eprint = {1602.02282},
        eprinttype = {arxiv},
        file = {/Users/jakob/Documents/Zotero/Sønderby et al - 2016 - Ladder Variational Autoencoders.pdf},
        keywords = {⛔ No DOI found,Computer Science - Machine Learning,Statistics - Machine Learning},
        langid = {english},
        primaryClass = {cs, stat}
      }
      
</script>